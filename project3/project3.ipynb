{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f826108",
   "metadata": {},
   "source": [
    "# Project 3: Decision Trees & Random Forests\n",
    "## This Project gives extra points for the final grade\n",
    "\n",
    "## **Due 29.01.2026 at 4 PM**\n",
    "\n",
    "## Overview\n",
    "\n",
    "### **Submit your project solution as a group of 2-4 people.**\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. **Implement Decision Tree** (2.5 Points)\n",
    "- Implement a decision tree classifier from scratch using the ID3 algorithm.\n",
    "- You are free to add additional functionalities to improve the performance of your decision tree.\n",
    "\n",
    "2. **Implement Random Forest** (2 Points)\n",
    "- Implement a random forest classifier from scratch using your decision tree implementation.\n",
    "- You are free to choose the hyperparameters of your random forest implementation.\n",
    "- You are free to add any additional functionalities to improve the performance of your random forest.\n",
    "\n",
    "3. **Testing & Evaluation** (0.5 Points)\n",
    "- Train and tune hyperparameters of your decision tree and random forest implementations using the training set (you can use sklearn for hyperparameter tuning).\n",
    "- You **must reach at least 95.5% accuracy** on the testing set with your final model.\n",
    "- The best performing model will be awarded a prize in the class.\n",
    "\n",
    "### **NOTE**: \n",
    "#### You are not allowed to use sklearn, pytorch or similar Deep Learning frameworks for the algorithm implementations. For data splitting, scoring and hyperparameter tuning sklearn is allowed. Numpy is allowed everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9555e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98d6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this part\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target) # 0 = malignant, 1 = benign\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374fae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a decision tree classifier from scratch using the ID3 algorithm.\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        # feature index and threshold used for splitting\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        # left and right child nodes\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf(self):\n",
    "        # check if this node is a leaf\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeID3:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, n_features=None):\n",
    "        # stopping conditions and feature subsampling\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "        y = y.values if isinstance(y, pd.Series) else y\n",
    "        self.n_features = X.shape[1] if self.n_features is None else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples = X.shape[0]\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # stopping criteria\n",
    "        if depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        # randomly select features (for random forest compatibility)\n",
    "        feat_idxs = np.random.choice(X.shape[1], self.n_features, replace=False)\n",
    "        best_feat, best_thresh, best_gain = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # stop splitting if information gain is too small\n",
    "        if best_gain <= 1e-7:\n",
    "            return Node(value=self._most_common_label(y))\n",
    "\n",
    "        # split data and grow subtrees\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)\n",
    "\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1\n",
    "        split_idx, split_thresh = None, None\n",
    "\n",
    "        # try all selected features and possible thresholds\n",
    "        for feat in feat_idxs:\n",
    "            X_col = X[:, feat]\n",
    "            values = np.unique(X_col)\n",
    "            thresholds = (values[:-1] + values[1:]) / 2\n",
    "\n",
    "            for thr in thresholds:\n",
    "                gain = self._information_gain(y, X_col, thr)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat\n",
    "                    split_thresh = thr\n",
    "\n",
    "        return split_idx, split_thresh, best_gain\n",
    "\n",
    "    def _information_gain(self, y, X_col, threshold):\n",
    "        # entropy before split\n",
    "        parent_entropy = self._entropy(y)\n",
    "        left, right = self._split(X_col, threshold)\n",
    "\n",
    "        # invalid split\n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "\n",
    "        # weighted entropy after split\n",
    "        n = len(y)\n",
    "        e_left = self._entropy(y[left])\n",
    "        e_right = self._entropy(y[right])\n",
    "        child_entropy = (len(left)/n) * e_left + (len(right)/n) * e_right\n",
    "\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        # compute entropy of labels\n",
    "        hist = np.bincount(y)\n",
    "        probs = hist / len(y)\n",
    "        probs = probs[probs > 0]\n",
    "        return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "    def _split(self, X_col, threshold):\n",
    "        # split indices based on threshold\n",
    "        left = np.where(X_col <= threshold)[0]\n",
    "        right = np.where(X_col > threshold)[0]\n",
    "        return left, right\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        # return majority class\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        # predict class for each sample\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "        return np.array([self._traverse(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse(self, x, node):\n",
    "         # traverse tree until leaf is reached\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse(x, node.left)\n",
    "        return self._traverse(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c9d057-ed06-4d72-b1bb-12221e7f836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a random forest classifier from scratch using decision tree implementation.\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_trees=50, max_depth=10, min_samples_split=2, n_features=None):\n",
    "        # forest hyperparameters\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_trees):\n",
    "            # create and train each tree on a bootstrap sample using decision tree implemented above\n",
    "            tree = DecisionTreeID3(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                n_features=self.n_features\n",
    "            )\n",
    "\n",
    "            idxs = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_sample = X.iloc[idxs]\n",
    "            y_sample = y.iloc[idxs]\n",
    "\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # collect predictions from all trees\n",
    "        preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        preds = preds.T\n",
    "        return np.array([np.bincount(row).argmax() for row in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f80f036-f16d-46ee-bdd5-b778abb4eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Tuning...\n",
      "Tested: n_trees=20, depth=8, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=20, depth=8, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=20, depth=9, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=20, depth=9, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=20, depth=10, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=20, depth=10, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=50, depth=8, feats=5 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=50, depth=8, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=50, depth=9, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=50, depth=9, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=50, depth=10, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=50, depth=10, feats=10 | Val Accuracy: 94.51%\n",
      "Tested: n_trees=100, depth=8, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=100, depth=8, feats=10 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=100, depth=9, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=100, depth=9, feats=10 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=100, depth=10, feats=5 | Val Accuracy: 95.60%\n",
      "Tested: n_trees=100, depth=10, feats=10 | Val Accuracy: 95.60%\n",
      "----------------------------------------\n",
      "Best validation accuracy: 95.60%\n",
      "Best parameters: (20, 8, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train and tune hyperparameters of decision tree and random forest implementations using the training set.\n",
    "\n",
    "# Hyperparameter Search Space\n",
    "n_trees_options = [20, 50, 100]\n",
    "max_depth_options = [8, 9, 10]\n",
    "n_features_options = [int(np.sqrt(X.shape[1])), 10]\n",
    "\n",
    "# Further splitting training set into train and validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "\n",
    "for n_trees in n_trees_options:\n",
    "    for depth in max_depth_options:\n",
    "        for n_feats in n_features_options:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_trees=n_trees,\n",
    "                max_depth=depth,\n",
    "                n_features=n_feats\n",
    "            )\n",
    "            rf.fit(X_tr, y_tr)\n",
    "            preds = rf.predict(X_val)\n",
    "            acc = accuracy_score(y_val, preds)\n",
    "\n",
    "            print(f\"Tested: n_trees={n_trees}, depth={depth}, feats={n_feats} | Val Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = (n_trees, depth, n_feats)\n",
    "                best_model = rf\n",
    "                \n",
    "print(\"-\" * 40)\n",
    "print(f\"Best validation accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af4948dd-85f5-4992-8eb3-43984a20a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on the test set\n",
    "best_model.fit(X_train, y_train)\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"Final Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc941fd-7fdd-40ee-a550-1cdc231c6b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349b1da-c3d9-4dba-9ae8-cbba944087af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2078e31-621d-4f7d-bee5-f2ef546e2909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca6e5a-6010-4231-b396-9eaec7ca6853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
